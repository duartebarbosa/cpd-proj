\documentclass[times, 10pt,twocolumn]{article} 
\usepackage{latex8}
\usepackage{url}
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % Allows the direct input of letters with accents
\usepackage{indentfirst}
\usepackage{algpseudocode}
\usepackage{amsmath}

\pagestyle{empty}

\begin{document}

\title{Parallel and Distributed Computing Project Report}

\author{
Duarte Barbosa, 65893\\
duarte.barbosa@ist.utl.pt\\
\and
Departamento de Engenharia Informática, DEI\\
Instituto Superior Técnico, IST\\
Lisboa, Portugal\\
}

\maketitle
\thispagestyle{empty}

This project consists of a serial, an openMP, an openMPI and an openMPI+openMP implementation of a given problem. The global objective of openMP is to provide a simple and flexible interface that permits developing parallel applications without having to entirely rewrite existing source code. Message Passing Interface (MPI) is a standardized and portable message-passing system design to function on a wide variety of parallel computers. Similarly, both standards define the syntax and semantics of a core of library routines useful to a wide range of users writing portable message-passing programs in Fortran 77 or the C programming language.

Unfortunately, parallelization comes at a price: scalability is limited by memory architecture and the incurring bigger overhead is only worth if the dataset in consideration is large enough. The serial version of the code was highly optimized before starting work on the parallel approach. This fact turned a baseline comparison much more concise – and harder to beat - when comparing to the improved openMP/openMPI version.

For a more acurate measure of the speedup achieved with all the implementations, a lot of profiling was done to the code. It was found that the initial population of the used data structures was indeed a section where a lot of time was spent and therefore, the speedup would not be very acurate if this time was considered (or not constant and/or small). To acomplish a better time in this area, naive pow() and strtod() functions were re-implemented with special attention to performance which gave a very different result.

Given the original serial source, the effort on parallelization was put on the computation and ordering/iteration of the cabinets and the distance calculation in the project.

Initially, all of the possible code sequences were indeed parallelized; even the calls to memory allocation functions ((c)malloc(), free() or memset()). However, this turned to be highly inefficient: all these functions depend on a shared resource manipulation (heap) which actually slowed down the whole benchmark. Moreover, given openMP strict requirements, some parts of the code (eg. The opening or closing of the files) couldn't be changed due to the return instructions of the corresponding if statement. Obviously, some code must be run sequentially; however, efforts were made to turn this bit small enough so that no difference was observed in runtime. In the openMPI version, a different approach was taken: the master thread will perform all the IO needed and then send the required data to each slave.

	The original source was codified to take advantage of a simpler interpretation and a serial-yet-quick implementation of the problem: after the initialization of the data structures and setting of the relevant information, we would start by calculating a centroid for each cabinet and given a distance to a document, move that document to the “closest” cabinet, looping until all the documents are correctly ordered. In this context, the distance and centroid are shared data structures (in the local scope of process function) which could not be easily parallelized. However, it was possible to do so recurring to a few tricks. Whenever the data dependency was avoidable, the code was changed to take advantage of this. Moreover, through the collapse clauses (and an extra variable) the code could be divided in chunks and fully parallelized. During tests, there was no noticiable difference in the scheduling used for the parallelization and most of the times, any extra clause would actually hurt the overall performance of the executable. After some experiments, there was a noticiable gap in the overall performance. The distance loop was not being parallelized which cause a dramatic performance drop. The new openMP version takes full advantage of this, increasing the overall speedup to circa 2.5 times in most testing scenarios. Given such "bad" serial implementation, both openMP and openMPI change a lot of the visible code presenting now new data structures and an overall different algorithm.

	In summary, the original serial algorithm was not though in a future-proof way, where parallelization could occur in simple ways.
	Then, the only worth parallelization – when possible, without defacing the original algorithm –  was on the code that computes the distance of the documents and the cabinet's centroid. The aforementioned code relies on various for loops, iterating on the documents, the subjects and the cabinets. Given the actual serial code, the parallelization was made in the subjects and cabinets loops – this fact implies that the performance of the parallel execution is dependent on the number of subjects and cabinets instead of the number of documents. Even than, this approach only makes sense for moderately large datasets with lots of subjects and/or cabinets. Comparatively, for small datasets (like the first example inputs released), effort was taken to make a compromise between serial and parallel executions. Instead of using bloated conditional omp pragma code, the choice made was to not over optimize/parallelize and to be able to obtain similar speed for both of them. Unfortunately, as good it may look, my tests proved that due to cache misses this code was indeed inefficient and took some extra time.

	Obviously, given a not-easily-parallelizable serial implementation to begin with, every forward move was very difficult. Instead, major rework was done to the openMP version, which now comprises a lot faster algorithm and a new data structure which causes a lot less page faults as the previous implementation.

	The openMPI version relies on the aforementioned implementation and divides the calculation of the centroid between different machines. Unfortunately, at the time of the final delivery, this implementation was a bit poor. The overhead caused by all the communication needed between different processes slowed down the execution giving in most small tests a even worse performance than the serial implementation. Obviously, for this kind of work, a machine-parallelized code does not matter at all; communication involves time and the speedup drops a lot. For bigger datasets, the performance increases a bit but nothing very noticiable. This was mainly due to the need of implicit barrier between centroid and distance calculation (for synchronization purposes), and no parallelization to some critical parts of the implementation.	

	Regarding some specific optimizations, precise instructions were given to the compiler for inlining methods and to put iteration variables (or frequently accessed ones) on CPU registers. It was discovered that certain glibc functions were a lot heavier and slower than the naive ones so they were replaced by custom versions. Efforts were made to develop concise, clean and fast code taking in consideration some of the characteristics of the underlying architecture, namely word size and cache hits/misses.

	My expectations were that the speedup would indeed be much higher. The decomposition used was not the best. I changed the serial implementation several times to gain a bit more knowledge about the dependencies and to be able explain such a poor result. Some of the most common pitfalls of OpenMP were learned the hard way and now, they would be carefully addressed before starting parallelizing code. Regarding the openMPI version, the results are not satisfactory. The communication was the biggest problem and I could not conceive a better algorithm for parallelizing code. In contrast, the openMPI+openMP version borrows some pragmas to the openMP implementation and although not faster than the previous mentioned code, it's faster than the serial solution.


\end{document}
